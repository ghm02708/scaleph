# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## 参考 https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

## Create default rules for monitoring the cluster
defaultRules:
  create: true
  rules:
    alertmanager: false
    etcd: false
    configReloaders: false
    # 启用 watchdog，测试 alert 通道
    general: true
    k8sContainerCpuUsageSecondsTotal: false
    k8sContainerMemoryCache: false
    k8sContainerMemoryRss: false
    k8sContainerMemorySwap: false
    k8sContainerResource: false
    k8sContainerMemoryWorkingSetBytes: false
    k8sPodOwner: false
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: false
    kubeProxy: false
    kubePrometheusGeneral: false
    kubePrometheusNodeRecording: false
    kubernetesApps: false
    kubernetesResources: false
    kubernetesStorage: false
    kubernetesSystem: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false
    kubeStateMetrics: false
    network: false
    node: false
    nodeExporterAlerting: false
    nodeExporterRecording: false
    prometheus: false
    prometheusOperator: false
    windows: false

## Provide custom recording or alerting rules to be deployed into the cluster.
##
#additionalPrometheusRulesMap:
#  rule-name:
#    groups:
#    - name: example.rules
#      rules:
#      - alert: ExampleAlert
#        expr: flink_k8soperator_namespace_Count > 0
#        for: 1m
#        labels:
#          severity: critical
#        annotations:
#          summary: An example rule for test.

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
alertmanager:
  ## Deploy alertmanager
  enabled: true
  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  config:
    route:
      group_by: [ 'namespace' ]
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 10s
      receiver: 'webhook'
      routes:
        - receiver: 'webhook'
    receivers:
      - name: 'webhook'
        webhook_configs:
          - send_resolved: false
            url: http://10.10.18.163:8868/carp/api/carp/alert

  ## Configuration for Alertmanager service
  service:
    ## Port for Alertmanager Service to listen on
    port: 9093
    ## To be used with a proxy extraContainer port
    targetPort: 9093
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    nodePort: 30903
    ## Service type
    type: NodePort
  ## Configuration for creating a ServiceMonitor for AlertManager
  serviceMonitor:
    ## If true, a ServiceMonitor will be created for the AlertManager service.
    selfMonitor: false

## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
## Grafana isn't managed by prometheus
grafana:
  enabled: false

## Flag to disable all the kubernetes component scrapers
kubernetesServiceMonitors:
  enabled: false

## Component scraping the kube api server
kubeApiServer:
  enabled: false

## Component scraping the kubelet and kubelet-hosted cAdvisor
kubelet:
  enabled: false

## Component scraping the kube controller manager
kubeControllerManager:
  enabled: false

## Component scraping coreDns. Use either this or kubeDns
coreDns:
  enabled: false

## Component scraping kubeDns. Use either this or coreDns
kubeDns:
  enabled: false

## Component scraping etcd
kubeEtcd:
  enabled: false

## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false

## Component scraping kube proxy
kubeProxy:
  enabled: false

## Component scraping kube state metrics
##
kubeStateMetrics:
  enabled: false

## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
  enabled: false


## Manages Prometheus and Alertmanager components
prometheusOperator:
  enabled: true
  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
  namespaces:
    releaseNamespace: false
  kubeletService:
    ## If true, the operator will create and maintain a service for scraping kubelets
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
    enabled: false
  ## Create a servicemonitor for the operator
  serviceMonitor:
    ## If true, create a serviceMonitor for prometheus operator
    selfMonitor: false
  ## Resource limits & requests
  resources:
    limits:
      cpu: 100m
      memory: 100Mi
    requests:
      cpu: 100m
      memory: 100Mi

## Deploy a Prometheus instance
prometheus:
  enabled: true
  ## Configuration for Prometheus service
  service:
    ## Port for Prometheus Service to listen on
    port: 9090
    ## To be used with a proxy extraContainer port
    targetPort: 9090
    ## Port for Prometheus Reloader to listen on
    reloaderWebPort: 8080
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    nodePort: 30090
    ## Service type
    type: NodePort

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  prometheusSpec:
    ## ServiceMonitors to be selected for target discovery.
    ## If {}, select all ServiceMonitors
    serviceMonitorSelector: { }
    ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
    #      matchLabels:
    #        group: flink-jobs
    #        platform: scaleph

    ## PodMonitors to be selected for target discovery.
    ## If {}, select all PodMonitors
    podMonitorSelector: { }
    ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
    #      matchLabels:
    #        group: flink-jobs
    #        platform: scaleph

    ## Interval between consecutive scrapes.
    ## Defaults to 30s.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
    scrapeInterval: "2s"
    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the PrometheusRule resources created
    ruleSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    serviceMonitorSelectorNilUsesHelmValues: true
    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    podMonitorSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the probes created
    probeSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the scrapeConfigs created
    scrapeConfigSelectorNilUsesHelmValues: false
    ## How long to retain metrics
    retention: 1d
    ## Maximum size of metrics
    retentionSize: ""
    ## Resource limits & requests
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

  additionalServiceMonitors:
    ## Name of the ServiceMonitor to create
    - name: flink-kubernetes-operator-job-metrics
      ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
      ## the chart
      additionalLabels:
        group: flink-kubernetes-operator-job-metrics
        platform: scaleph
      ## labels to transfer from the kubernetes pods to the target
      ## todo 后续在 scaleph 可以增加更多的 labels 到 flink pods 中，通过这种方式传到 prometheus 和 grafana 中去
      podTargetLabels:
        - app
        - component
      ## Label selector for services to which this ServiceMonitor applies
      selector:
        matchLabels:
          app: flink-kubernetes-operator-job-metrics
          component: metrics
          platform: scaleph
      ## Endpoints of the selected service to be monitored
      endpoints:
        ## Name of the endpoint's service port
        - port: prom-metrics
          interval: 10s
      ## HTTP path to scrape for metrics
      # path: /metrics
      #    - name: flink-kubernetes-operator-service-metrics
      #      additionalLabels:
      #        group: flink-kubernetes-operator-metrics
      #        platform: scaleph
      ## todo 后续在 scaleph 可以增加更多的 labels 到 flink pods 中，通过这种方式传到 prometheus 和 grafana 中去
  #      podTargetLabels: []
  #      selector:
  #        matchLabels:
  #          app: flink-kubernetes-operator-metrics
  #          component: metrics
  #          platform: scaleph
  #      endpoints:
  #        - port: prom-metrics
  #          interval: 10s

  additionalPodMonitors:
    ## Name of the PodMonitor to create
    ##
    - name: "flink-kubernetes-operator-metrics"

      ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
      ## the chart
      ##
      additionalLabels:
        group: flink-kubernetes-operator-pod-metrics
        platform: scaleph

      ## Pod label for use in assembling a job name of the form <label value>-<port>
      ## If no label is specified, the pod endpoint name is used.
      ##
      # jobLabel: ""

      ## Label selector for pods to which this PodMonitor applies
      ##
      selector:
        matchLabels:
          app.kubernetes.io/name: flink-kubernetes-operator

      ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
      ##
      # podTargetLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      # sampleLimit: 0

      ## Namespaces from which pods are selected
      ##
      # namespaceSelector:
      ## Match any namespace
      ##
      # any: false

      ## Explicit list of namespace names to select
      ##
      # matchNames: []

      ## Endpoints of the selected pods to be monitored
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint
      ##
      podMetricsEndpoints:
        - port: metrics